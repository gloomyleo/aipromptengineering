# ðŸ“ AI Security Labs â€“ Trainee Report Template

Use this template to capture your findings while running the labs.

---

## Participant
- **Name:**  
- **Date:**  
- **Team/Function:**  

## Environment
- **OS / Version:**  
- **Python Version:**  
- **CPU/GPU:** (CPU is fine)  
- **Anything unusual:**  

---

## Lab 1 â€“ Prompt Injection
**Goal:** Understand how prompts can subvert LLM guardrails; test defenses.

### Prompts Attempted
| Prompt | Baseline Output (vulnerable) | Mitigated Output | Notes |
|---|---|---|---|
|  |  |  |  |
|  |  |  |  |

**Observations:**  
- What worked? What didnâ€™t?  
- Any leakage or unsafe responses?  

**Controls/Mitigations to Carry Forward:**

---

## Lab 2 â€“ Adversarial Examples (Digits)
**Goal:** Generate FGSM examples that change a classifierâ€™s decision.

### Runs
| Epsilon | Success? | Comments |
|---:|:---:|---|
| 0.1 |  |  |
| 0.5 |  |  |
| 1.0 |  |  |

**Attach:** `adv_result.png` (original vs adversarial)  

**Observations:**  
- Visual distortion vs. attack success tradeoff  
- Which epsilon worked best and why?

---

## Lab 3 â€“ Model Extraction (Iris API)
**Goal:** Approximate a model via API queries; test mitigations.

### Baseline (Vulnerable)
- **Collected samples:**  
- **Clone accuracy (holdout):**  

### Mitigated (API key + rate limit + reduced outputs)
- **429s observed?**  
- **Clone accuracy (holdout):**  
- **API key used:** yes / no

**Logs/Screenshots:**  
- Paste key excerpts showing 401/429 and attacker behavior.

**Mitigations to Adopt:**  
-  â€¦

---

## Lab 4 â€“ Data Poisoning Detection
**Goal:** Detect label flips/outliers; retrain and compare.

### Results
| Poison % | Contamination | Clean Acc | Poisoned Acc | Cleaned Acc | Removed Samples |
|---:|---:|---:|---:|---:|---:|
| 0.1 | 0.1 |  |  |  |  |

**Observations:**  
- How sensitive are results to parameters?  
- Any false positives from the anomaly detector?

---

## Framework Mapping (tick what you addressed)
- **NIST AI RMF:** GV.3 â˜ GV.4 â˜ MP.3 â˜ MP.4 â˜ MP.5 â˜ MS.1 â˜ MS.2 â˜ MS.3 â˜ MG.2 â˜ MG.3 â˜ MG.4 â˜ MG.5 â˜
- **MITRE ATLAS:** Prompt Injection â˜ Adversarial Examples â˜ Model Extraction â˜ Data Poisoning â˜ Model Integrity Compromise â˜

---

## Final Notes & Next Actions
- What would you harden in production?
- Any follow-up experiments?

---

**Downloads:**  
- PDF: [AI_Sec_Lab_Report_Template.pdf](assets/AI_Sec_Lab_Report_Template.pdf)
- Markdown: [report-template.md](report-template.md)

> To generate a DOCX, use Pandoc locally:
```bash
pandoc docs/report-template.md -o AI_Sec_Lab_Report_Template.docx
```
